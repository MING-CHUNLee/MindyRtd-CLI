# ==================================
# Mindy RStudio CLI Configuration
# ==================================
# Copy this file to .env and fill in your values
# NEVER commit .env to version control!

# ==================================
# LLM Provider Configuration
# ==================================

# Choose your LLM provider: openai, anthropic, azure, ollama
# If not set, will auto-detect from available API keys
# LLM_PROVIDER=openai

# API Keys (use the one matching your provider)
# Following LangChain naming conventions
OPENAI_API_KEY=sk-your-openai-key-here
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
# AZURE_OPENAI_API_KEY=your-azure-key-here

# ==================================
# Model Configuration
# ==================================

# Model to use (optional, defaults based on provider)
# OpenAI: gpt-4, gpt-4-turbo, gpt-3.5-turbo
# Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229
# LLM_MODEL=gpt-4

# Maximum tokens in response
# LLM_MAX_TOKENS=4096

# Request timeout in milliseconds
# LLM_TIMEOUT=30000

# ==================================
# Custom Endpoints (Optional)
# ==================================

# OpenAI-compatible endpoint (for proxies or custom deployments)
# OPENAI_API_BASE=https://api.openai.com/v1/chat/completions

# Azure OpenAI endpoint (required if using Azure)
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/

# Ollama host (for local LLM)
# OLLAMA_HOST=http://localhost:11434/api/chat

# ==================================
# Application Settings
# ==================================

# Enable debug mode
# DEBUG=false

# Log level: debug, info, warn, error
# LOG_LEVEL=info
