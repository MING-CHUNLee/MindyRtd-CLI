/**
 * Controller: LLM Controller
 *
 * Handles communication with LLM APIs (OpenAI, Anthropic, etc.)
 *
 * Architecture References:
 * - LangChain: Provider abstraction pattern
 * - Vercel AI SDK: Unified interface for multiple providers
 * - Clean Architecture: Gateway/Adapter pattern
 *
 * Best Practices:
 * - API keys loaded from environment variables (never hardcoded)
 * - Provider-agnostic interface
 * - Automatic retry with exponential backoff
 * - Proper error handling and typing
 * - Input validation before API calls
 */

import { GeneratedPrompt } from '../types/prompt-context';
import { LLMConfig, getLLMConfigFromEnv, LLMProvider } from '../config';
import { LLM } from '../config/constants';

// ============================================
// Error Classes
// ============================================

export class LLMValidationError extends Error {
    constructor(message: string) {
        super(message);
        this.name = 'LLMValidationError';
    }
}

export class LLMAPIError extends Error {
    constructor(
        message: string,
        public readonly provider: LLMProvider,
        public readonly statusCode?: number
    ) {
        super(message);
        this.name = 'LLMAPIError';
    }
}

// ============================================
// Types
// ============================================

export interface LLMMessage {
    role: 'system' | 'user' | 'assistant';
    content: string;
}

export interface LLMRequest {
    /** System prompt (generated by context-builder) */
    systemPrompt: string;
    /** User's message or code to analyze */
    userMessage: string;
    /** Conversation history (optional) */
    history?: LLMMessage[];
    /** Override default model for this request */
    model?: string;
}

export interface LLMResponse {
    /** LLM's response content */
    content: string;
    /** Token usage information */
    usage?: {
        promptTokens: number;
        completionTokens: number;
        totalTokens: number;
    };
    /** Model used for response */
    model: string;
    /** Provider used */
    provider: LLMProvider;
    /** Response time in ms */
    responseTimeMs?: number;
}

export interface LLMControllerOptions {
    /** Override config from environment */
    config?: Partial<LLMConfig>;
    /** Enable retry on failure */
    enableRetry?: boolean;
    /** Maximum retry attempts */
    maxRetries?: number;
}

// Internal types for API responses
interface OpenAIResponse {
    choices: Array<{ message?: { content?: string } }>;
    usage?: { prompt_tokens: number; completion_tokens: number; total_tokens: number };
    model: string;
    error?: { message?: string };
}

interface AnthropicResponse {
    content: Array<{ text?: string }>;
    usage?: { input_tokens: number; output_tokens: number };
    model: string;
    error?: { message?: string };
}

// ============================================
// LLM Controller
// ============================================

export class LLMController {
    private config: LLMConfig;
    private enableRetry: boolean;
    private maxRetries: number;

    /**
     * Create an LLM Controller
     * 
     * @example
     * // Use environment configuration (recommended)
     * const controller = LLMController.fromEnv();
     * 
     * // Or with custom options
     * const controller = new LLMController({
     *     config: { model: 'gpt-3.5-turbo' }
     * });
     */
    constructor(options: LLMControllerOptions = {}) {
        // Load config from environment, allow overrides
        const envConfig = getLLMConfigFromEnv();
        this.config = {
            ...envConfig,
            ...options.config,
        };
        this.enableRetry = options.enableRetry ?? true;
        this.maxRetries = options.maxRetries ?? 3;
    }

    /**
     * Factory method: Create controller from environment variables
     */
    static fromEnv(): LLMController {
        return new LLMController();
    }

    /**
     * Send a prompt to the LLM and get a response
     */
    async sendPrompt(request: LLMRequest): Promise<LLMResponse> {
        this.validateRequest(request);

        const messages: LLMMessage[] = [
            { role: 'system', content: request.systemPrompt },
            ...(request.history || []),
            { role: 'user', content: request.userMessage },
        ];

        const model = request.model || this.config.model;
        const startTime = Date.now();

        const result = await this.executeWithRetry(() =>
            this.sendToProvider(messages, model)
        );

        return {
            ...result,
            responseTimeMs: Date.now() - startTime,
        };
    }

    // ============================================
    // Validation
    // ============================================

    /**
     * Validate request parameters before sending to API
     */
    private validateRequest(request: LLMRequest): void {
        // Validate system prompt
        if (!request.systemPrompt?.trim()) {
            throw new LLMValidationError('System prompt is required and cannot be empty');
        }

        if (request.systemPrompt.length > LLM.MAX_SYSTEM_PROMPT_LENGTH) {
            throw new LLMValidationError(
                `System prompt exceeds maximum length of ${LLM.MAX_SYSTEM_PROMPT_LENGTH} characters`
            );
        }

        // Validate user message
        if (!request.userMessage?.trim()) {
            throw new LLMValidationError('User message is required and cannot be empty');
        }

        if (request.userMessage.length > LLM.MAX_USER_MESSAGE_LENGTH) {
            throw new LLMValidationError(
                `User message exceeds maximum length of ${LLM.MAX_USER_MESSAGE_LENGTH} characters`
            );
        }

        // Validate history messages if present
        if (request.history) {
            for (const msg of request.history) {
                if (!msg.content?.trim()) {
                    throw new LLMValidationError('History messages cannot have empty content');
                }
                if (!['system', 'user', 'assistant'].includes(msg.role)) {
                    throw new LLMValidationError(`Invalid message role: ${msg.role}`);
                }
            }
        }
    }

    /**
     * Convenience method: Send with GeneratedPrompt from context-builder
     */
    async sendWithContext(
        generatedPrompt: GeneratedPrompt,
        userMessage: string,
        history?: LLMMessage[]
    ): Promise<LLMResponse> {
        return this.sendPrompt({
            systemPrompt: generatedPrompt.systemPrompt,
            userMessage,
            history,
        });
    }

    /**
     * Analyze R code with environment context
     */
    async analyzeCode(
        generatedPrompt: GeneratedPrompt,
        rCode: string
    ): Promise<LLMResponse> {
        const userMessage = `Please analyze the following R code:\n\n\`\`\`r\n${rCode}\n\`\`\``;
        return this.sendWithContext(generatedPrompt, userMessage);
    }

    /**
     * Get current provider info (safe to log, no secrets)
     */
    getProviderInfo(): { provider: LLMProvider; model: string; endpoint: string } {
        return {
            provider: this.config.provider,
            model: this.config.model,
            endpoint: this.config.endpoint || '',
        };
    }

    // ============================================
    // Provider Router
    // ============================================

    private async sendToProvider(
        messages: LLMMessage[],
        model: string
    ): Promise<Omit<LLMResponse, 'responseTimeMs'>> {
        switch (this.config.provider) {
            case 'openai':
                return this.sendToOpenAI(messages, model);
            case 'anthropic':
                return this.sendToAnthropic(messages, model);
            case 'azure':
                return this.sendToAzure(messages, model);
            case 'ollama':
                return this.sendToOllama(messages, model);
            default:
                throw new Error(`Unsupported provider: ${this.config.provider}`);
        }
    }

    // ============================================
    // Provider Implementations
    // ============================================

    private async sendToOpenAI(
        messages: LLMMessage[],
        model: string
    ): Promise<Omit<LLMResponse, 'responseTimeMs'>> {
        const response = await this.fetchWithTimeout(this.config.endpoint!, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.config.apiKey}`,
            },
            body: JSON.stringify({
                model,
                messages,
                max_tokens: this.config.maxTokens,
            }),
        });

        if (!response.ok) {
            const errorData = await response.json() as OpenAIResponse;
            throw new Error(`OpenAI API error: ${errorData.error?.message || response.statusText}`);
        }

        const data = await response.json() as OpenAIResponse;

        return {
            content: data.choices[0]?.message?.content || '',
            usage: data.usage ? {
                promptTokens: data.usage.prompt_tokens,
                completionTokens: data.usage.completion_tokens,
                totalTokens: data.usage.total_tokens,
            } : undefined,
            model: data.model,
            provider: 'openai',
        };
    }

    private async sendToAnthropic(
        messages: LLMMessage[],
        model: string
    ): Promise<Omit<LLMResponse, 'responseTimeMs'>> {
        const systemMessage = messages.find(m => m.role === 'system')?.content || '';
        const conversationMessages = messages
            .filter(m => m.role !== 'system')
            .map(m => ({ role: m.role, content: m.content }));

        const response = await this.fetchWithTimeout(this.config.endpoint!, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'x-api-key': this.config.apiKey,
                'anthropic-version': '2023-06-01',
            },
            body: JSON.stringify({
                model,
                max_tokens: this.config.maxTokens,
                system: systemMessage,
                messages: conversationMessages,
            }),
        });

        if (!response.ok) {
            const errorData = await response.json() as AnthropicResponse;
            throw new Error(`Anthropic API error: ${errorData.error?.message || response.statusText}`);
        }

        const data = await response.json() as AnthropicResponse;

        return {
            content: data.content[0]?.text || '',
            usage: data.usage ? {
                promptTokens: data.usage.input_tokens,
                completionTokens: data.usage.output_tokens,
                totalTokens: data.usage.input_tokens + data.usage.output_tokens,
            } : undefined,
            model: data.model,
            provider: 'anthropic',
        };
    }

    private async sendToAzure(
        messages: LLMMessage[],
        model: string
    ): Promise<Omit<LLMResponse, 'responseTimeMs'>> {
        // Azure OpenAI uses a different endpoint format
        const response = await this.fetchWithTimeout(this.config.endpoint!, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'api-key': this.config.apiKey,
            },
            body: JSON.stringify({
                messages,
                max_tokens: this.config.maxTokens,
            }),
        });

        if (!response.ok) {
            const errorData = await response.json() as OpenAIResponse;
            throw new Error(`Azure OpenAI error: ${errorData.error?.message || response.statusText}`);
        }

        const data = await response.json() as OpenAIResponse;

        return {
            content: data.choices[0]?.message?.content || '',
            usage: data.usage ? {
                promptTokens: data.usage.prompt_tokens,
                completionTokens: data.usage.completion_tokens,
                totalTokens: data.usage.total_tokens,
            } : undefined,
            model: model,
            provider: 'azure',
        };
    }

    private async sendToOllama(
        messages: LLMMessage[],
        model: string
    ): Promise<Omit<LLMResponse, 'responseTimeMs'>> {
        // Ollama uses a different format
        const response = await this.fetchWithTimeout(this.config.endpoint!, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({
                model,
                messages,
                stream: false,
            }),
        });

        if (!response.ok) {
            throw new Error(`Ollama error: ${response.statusText}`);
        }

        const data = await response.json() as { message?: { content?: string } };

        return {
            content: data.message?.content || '',
            model,
            provider: 'ollama',
        };
    }

    // ============================================
    // Utilities
    // ============================================

    private async fetchWithTimeout(
        url: string,
        options: RequestInit
    ): Promise<Response> {
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), this.config.timeout);

        try {
            const response = await fetch(url, {
                ...options,
                signal: controller.signal,
            });
            return response;
        } finally {
            clearTimeout(timeoutId);
        }
    }

    private async executeWithRetry<T>(
        fn: () => Promise<T>,
        attempt = 1
    ): Promise<T> {
        try {
            return await fn();
        } catch (error) {
            if (!this.enableRetry || attempt >= this.maxRetries) {
                throw error;
            }

            // Exponential backoff
            const delay = Math.min(1000 * Math.pow(2, attempt - 1), 10000);
            await this.sleep(delay);

            return this.executeWithRetry(fn, attempt + 1);
        }
    }

    private sleep(ms: number): Promise<void> {
        return new Promise(resolve => setTimeout(resolve, ms));
    }
}

// ============================================
// Factory Functions
// ============================================

/**
 * Create controller from environment (recommended)
 */
export function createLLMController(options?: LLMControllerOptions): LLMController {
    return new LLMController(options);
}

export default LLMController;
